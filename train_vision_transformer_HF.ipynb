{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complete Hugging Face tutorial: how to build and train a vision transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article serves as an all-in tutorial of the Hugging Face ecosystem. We will explore the different libraries developed by the Hugging Face team such as transformers and datasets. We will see how they can be used to develop and train transformers with minimum boilerplate code. To better elaborate the basic concepts, we will showcase the entire pipeline of building and training a Vision Transformer (ViT).\n",
    "\n",
    "I assume that you already are familiar with the architecture so we won’t analyze much about it. A few things to remember are:\n",
    "\n",
    "1. In ViT, we represent an image as a sequence of patches .\n",
    "\n",
    "1. The architecture resembles the original Transformer from the famous “Attention is all you need” paper.\n",
    "\n",
    "1. The model is trained using a labeled dataset following a fully-supervised paradigm.\n",
    "\n",
    "1. It is usually fine-tuned on the downstream dataset for image classification.\n",
    "\n",
    "If you are interested in a holistic view of the ViT architecture, visit one of our previous articles on the topic: [How the Vision Transformer (ViT) works in 10 minutes: an image is worth 16x16 words.](https://theaisummer.com/vision-transformer/)\n",
    "\n",
    "![source.gif](source.gif)\n",
    "\n",
    "Back to Hugging face which is the main objective of the article. We will strive to present the fundamental principles of the libraries covering the entire ML pipeline: from data loading to training and evaluation.\n",
    "\n",
    "Shall we begin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "The datasets library by Hugging Face is a collection of ready-to-use datasets and evaluation metrics for NLP. At the moment of writing this, the datasets hub counts over 900 different datasets. Let’s see how we can use it in our example.\n",
    "\n",
    "To load a dataset, we need to import the `load_dataset` function and load the desired dataset like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/usmanasim/Desktop/unomic/ViT/train_vision_transformer.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/usmanasim/Desktop/unomic/ViT/train_vision_transformer.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/usmanasim/Desktop/unomic/ViT/train_vision_transformer.ipynb#ch0000021?line=2'>3</a>\u001b[0m train_ds, test_ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mcifar10\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain[:5000]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest[:2000]\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we load only a portion of the CIFAR10 dataset. Using `load_dataset`, we can download datasets from the Hugging Face Hub, read from a local file, or load from in-memory data. We can also configure it to use a custom script containing the loading functionality.\n",
    "\n",
    "Typically, the dataset will be returned as a `datasets.Dataset` object which is nothing more than a table with rows and columns. Querying a row will return a python dictionary with keys corresponding to the column names and values to the value in this particular row-column cell. In other words, each row corresponds to a data-point and each column to a feature. We can get the entire structure of the dataset using `datasets.features`.\n",
    "\n",
    "A `Dataset` object is behaving like a Python list so we can query as we’d normally do with Numpy or Pandas:\n",
    "\n",
    "1. A single row is `dataset[3]`\n",
    "\n",
    "1. A batch is `dataset:[3:6]`\n",
    "\n",
    "1. A column is `dataset[‘feature_1’]`\n",
    "\n",
    "Everything is a Python object but that doesn’t mean that it can’t be converted into NumPy, pandas, PyTorch or TensorFlow. This can be very easily accomplished using `datasets.Dataset.set_format()`, where the format is one of `'numpy'`, `'pandas'`, `'torch'`, `'tensorflow'`.\n",
    "\n",
    "No need to say that there is also support for all types of operations. To name a few: `sort`, `shuffle`, `filter`, `train_test_split`, `shard`, `cast`, `flatten` and `map` . `map` is , of course, the main function to perform transformations and as you’d expect is parallelizable.\n",
    "\n",
    "In our example, we first need to split the training data into a training and a validation dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = splits['train']\n",
    "\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The datasets library also provides a wide list of metrics that can be used when training models. The main object here is a `datasets.Metric` and can be utilized into two ways:\n",
    "\n",
    "1. We can either load an existing metric from the Hub using `datasets.load_metric(‘metric_name’)`\n",
    "\n",
    "1. Or we can define a custom metric in a separate script and load it using: `load_metric('PATH/TO/MY/METRIC/SCRIPT')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "Transformers is the main library by Hugging Face. It provides intuitive and highly abstracted functionalities to build, train and fine-tune transformers. It comes with almost 10000 pretrained models that can be found on the Hub. These models can be built in Tensorflow, Pytorch or JAX (a very recent addition) and anyone can upload his own model.\n",
    "\n",
    "Alongside with our example code, we will dive a little deeper into the main classes and features of the transformers library.\n",
    "\n",
    "## Pipelines\n",
    "The `pipeline` abstraction is an intuitive and easy way to use a model for inference. They abstract most of the code from the library and provide a dedicated API for a variety of tasks. Examples include: `AutomaticSpeechRecognitionPipeline`, `QuestionAnsweringPipeline` , `TranslationPipeline` and more.\n",
    "\n",
    "The `pipeline` object lets us also define the pretrained model as well as the tokenizer, the feature extractor, the underlying framework and more. Tokenizer and feature extractors? What are those? Hold that thought for the next section.\n",
    "\n",
    "In our case, we can use the `transformers.ImageClassificationPipeline` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be used for inference. All we have to do is feed an image and we are good to go.\n",
    "\n",
    "However, in many cases, we also need to train or fine tune a model. Perhaps we also want better control on the entire pipeline. Therefore, we might need to develop the code ourselves. For educational purposes, this is what we’ll do here.\n",
    "\n",
    "# Preparing the dataset\n",
    "The first step to any ML lifecycle is to transform the dataset. In our case, we need to preprocess the CIFAR10 images so that we can feed them to our model. Hugging Face has two basic classes for data processing. Tokenizers and feature extractors.\n",
    "\n",
    "## Tokenizers\n",
    "In most NLP tasks, a `tokenizer` is our go-to solution. A tokenizer is mapping the text into tokens and then into numerical inputs that can be fed into the model. Each model comes with its own tokenizer that is based on the `PreTrainedTokenizer` class.\n",
    "\n",
    "Since we are dealing with images, we will not use a `Tokenizer` here. We will cover them more extensively in a future tutorial.\n",
    "\n",
    "## Feature Extractors\n",
    "However, we will make use of another class called feature extractors. A feature extractor is usually responsible for preparing input features for models that don’t fall into the standard NLP models. They are in charge of things such as processing audio files and manipulating images. Most vision models come with a complementary feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature extractor will resize every image to the resolution that the model expects and normalize the channels.\n",
    "Now we can define the entire processing functionality as depicted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "\n",
    "    images = examples['img']\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    inputs = feature_extractor(images=images)\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples\n",
    "\n",
    "from datasets import Features, ClassLabel, Array3D\n",
    "\n",
    "features = Features({\n",
    "    'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
